# 쿠버네티스

## 쿠버네티스란 ?

### 정의

**Kubernetes는 Container를 쉽고 빠르게 배포/확장하고 관리를 자동화 해주는 오픈소스 플랫폼입니다.** 글자가 너무 길어서 흔히 k8s 또는 kube라고 줄여서 부릅니다.

- [http://kubernetes.io](http://kubernetes.io/)
- 머신이 아니라 애플리케이션을 관리
  - Rolling updates, canary deploys, and blue-green deployments
- 확장성을 고려한 설계
  - 스케줄링, 스토리지, 네트워킹을 위한 플러그인을 제공하는 풍부한 에코 시스템
  - 여러 컨테이너에 걸쳐 클라이언트 요청을 분산
- 리눅스 재단에 의해 관리되는 오픈 소스 프로젝트
  - 구글의 경험과 내부 시스템에 의해 영감 받고 영향 받음
  - Go언어로 작성된 100% 오픈 소스



### 아키텍처

Kubernetes는 Master와 Node(worker)로 구성됩니다. Master에 API 서버와 상태 저장소(etcd)를 두고 각 Worker Node의 에이전트(Kubelet)와 통신하는 구조입니다.

쿠버네티스의 클러스터는 마스터노드(Master)와 워커노드(Node)를 가지고 있습니다. 



마스터용 컴포넌트

- ETCD :  분산 시스템에서 사용 할 수 있는 분산형 키-값 (Key-Value) 저장소 입니다. 쿠버네티스에서는 필요한 모든 데이터를 저장하는 데이터베이스 역할을 합니다.
- API server : 쿠버네티스 API를 사용 할 수 있도록 하는 컴포넌트입니다. 클러스터로 온 요청이 유효한지 검증하는 역할. 예를들어 클러스터의 특정 네임스페이스에 존재하는 디플로이먼트 목록 조회 요청을 받으면, 이 요청에 사용된 토큰이 해당 네임스페이스와 자원을 대상으로 요청을 실행할 권한이 있는 검사 한 뒤 요청을 처리합니다.
- Kube - Scheduler : 현재 클러스터 안에서 자원할당이 가능한 노드(워커노드) 중 알맞은 노드를 선택해서 새롭게 만든 파드를 실행합니다.
- Kube - Controller - Manager : 쿠버네티스는 파드들을 관리하는 컨트롤러가 있습니다. Kube-Controller-Manager는 컨트롤러 각각을 실행하는 컴포넌트입니다. 
- Cloud - Controller - Manager : 쿠버네티스의 컨트롤러들을 서비스와 연결해 관리하는 컴포넌트입니다. 



노드용 컴포넌트

- Kubelet : 클러스터 안 모든 노드에서 실행되는 에이전트입니다. 파드 컨테이너들의 실행을 직접 관리합니다. kubelet은 파드스펙(PodSpec)이라는 조건이 담긴 설정을 전달받아서 컨테이너를 실행하고 컨테이너가 정상적으로 실행되는지 헬스 체크를 진행합니다.
- Kube - Proxy : 쿠버네티스는 클러스터 안에 별도의 가상 네트워크를 설정하고 관리합니다. kube-proxy는 이런 가상 네트워크의 동작을 관리하는 컴포넌트입니다. 호스트의 네트워크 규칙을 관리하거나 연결을 전달 할 수도 있습니다.
- Container Runtime :  컨테이너 런타임은 실제로 컨테이너를 실행시킵니다. 가장 많이 알려진 런타임으로는 '도커'가 있고 containerd, runc같은 런타임도 지원합니다. 



### 주요 개념

#### Pod

- Pod는 쿠버네티스에서 관리하는 가장 작은 배포 단위, 쿠버네티스와 도커의 차이점은 도커는 컨테이너를 만들지만, 쿠버네티스는 대신 Pod를 만듭니다. Pod는 한 개 또는 여러개의 컨테이너를 포함합니다.
- Pod 내의 컨테이너들은 같은 스토리지 볼륨, 네트워크 인터페이스를 공유
- “Pod당 1개 Container” 모델이 가장 일반적인 Use Case입니다.


- Pod 생성과정

![pod_create2](쿠버1.png)



#### ReplicaSet

- Pod를 단독으로 만들면 Pod에 어떤 문제( 서버가 죽어서 Pod가 사라졌다던가)가 생겼을 때 자동으로 복구되지 않습니다. 이러한 Pod를 정해진 수만큼 복제하고 관리하는 것이 ReplicaSet입니다.
- ReplicaSet은 원하는 개수의 Pod를 유지하는 역할을 담당합니다. label을 이용하여 Pod를 체크하기 때문에 label이 겹치지 않게 신경써서 정의해야 합니다.
- ReplicaSet 생성예시
```sh
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: echo-rs
spec:
  replicas: 1
  selector:
    matchLabels:
      app: echo
      tier: app
  template:
    metadata:
      labels:
        app: echo
        tier: app
    spec:
      containers:
        - name: echo
          image: ghcr.io/subicura/echo:v1

* 자세한 명세는 .spec 필드에 설정합니다. .spec. template 필드에는 레플리카세트가 어떤 파드를 실행할지에 관한 정보를 설정합니다.

* .spec. template. spec. containers 필드는 하위에 .name, .image 필드를 이용해 컨테이너의 구체적인 명세를 설정합니다. 여기에서는 컨테이너의 이름을 echo, 사용할 컨테이너의 이미지는 ghcr.io/subicura/echo:v1으로 설정했습니다.

* .spec .replicas는 파드를 몇 개 유지할지 개수를 설정하는 필드입니다. 여기에서는 replicas: 1 로 설정했습니다.

* .spec .selector는 어떤 레이블( . labels)의 파드를 선택해서 관리할지를 설정합니다. 레이블을 기준으로 파드를 관리하므로 실행 중인 파드를 중단하거나 재시작하지 않고 레플리케이션 컨트롤러가 관리하는 파드를 변경 할 수 있습니다. 
그래서 처음 레플리케이션 컨트롤러를 생성 할 때 .spec .template. metadata. labels의 하위 필드 설정과 .spec .selector. matchLabels의 하위 필드 설정이 같아야 합니다. 
```

![replicaset_result](쿠버2.png)
replicas: 1 이므로 replicaset에 1개의 pod가 생성 된 것을 확인 할 수 있습니다.


#### Deployment

- Deployment는 쿠버네티스에서 가장 널리 사용되는 오브젝트
- ReplicaSet을 이용하여 Pod를 업데이트하고 이력을 관리하여 rollback하거나 특정버전으로 돌아 갈 수 있습니다.


#### Service

- Pod의 하나의 논리적 세트와 접근 정책을 정의하는 추상화
- 1개의 Endpoint로 노출 되는 1개 이상 pods의 모음
- service는 1개의 고정 IP 주소를 가지고 (Label에 의해 선택된) pods의 세트를 참조할 방법을 제공함

![1620970278256](쿠버3.png)



### 주요 명령어

```shell
// Pod 리스트를 출력
$ kubectl get pods

// Pod 리스트(+ 추가적인 정보 node 이름 등)를 출력
$ kubectl get pods -o wide

// 특정 <rc-name>의 정보를 출력
$ kubectl get replicationcontroller <rc-name>

// 모든 rc, service들 정보를 출력
$ kubectl get rc,services

// 모든 ds(daemon sets)에 대한 정보를 출력(uninitialized ds도 포함)
$ kubectl get ds --include-uninitialized

// 특정 Node(server01)에 배포된 Pod 정보를 출력
$ kubectl get pods --field-selector=spec.nodeName=server01

```



## 쿠버네티스 Cluster 서비스(네트워크)

### Cluster IP 구조

Cluster 환경에서는 다양한 IP가 부여되 제공되고 있으며, 각각의 IP는 제공 공간과 접근 범위에 따라 각기 다른 목적으로 사용됩니다. Cluster의 최소 빌딩 블록에 해당하는 Pod부터 IP를 부여하며 관리하여, 동일 역할의 여러 Pod의 집합체 역할을 수행하는 Service에 부여하는 서비스 IP(또는 ClusterIP)가 있고, Pod가 실질적으로 동작하는 Node 단위에서 관리되는 Node IP가 있습니다.



#### Pod IP

모든 Pod는 IP를 부여 받고, IP 기반으로 Pod 간에는 서로 통신이 가능합니다.

```sh
NAME                      READY     STATUS    RESTARTS   AGE       IP 
arsenal-assembler         1/1       Running   0          6d        192.168.10.134   
arsenal-collector         1/1       Running   0          6d        192.168.10.127
```

![1](쿠버4.png)

- Pod가 동작할 때 Linux 커널의 네트워크 Namespace가 생성되고, 이 Namespace는 가상의 네트워크 인터페이스(veth1234, veth5678)를 사용
- 노드의 물리적 네트워크 인터페이스(eth0)와 Pod가 연결되어 패킷이 전달이 이뤄짐
- 각 Pod 간에는 노드에 연결된 Bridge Interface에 연결되어 동일 노드에서 Pod 간 통신을 허용



#### Service IP

서비스 IP는 관련성 있는 Pod를 논리 단위로 그룹화 하여 IP를 부여 하는 방식입니다. 서비스 IP에 집합 된 Pod들에 접근하는 제공자에게 안정적인 IP 정보를 제공하며, Pod들에 대한 부하 분산 등도 제공합니다.

```sh
NAME                  TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)     AGE
arsenal-assembler     ClusterIP   172.30.10.3      <none>        80/TCP      6d
arsenal-collector     ClusterIP   172.30.123.226   <none>        80/TCP      7d
```

![2](쿠버5/6.png)

- Pod에는 Label을 정의하여, Service에서 Selector를 사용하여 집합체를 정의
- Service는 별도의 안정적이고 신뢰할 수 있는 IP 주소 할당
- port는 Service에서 제공하는 정보이고,  targetPort는 Pod와 연결되는 정보



#### Node IP

Node IP는 Pod가 실질적으로 동작하는 Node 단위에 부여 되는 IP이며, 일반적으로 하드웨어 장비에 제공하는 IP 방식과 동일하다.

```sh
NAME                  STATUS    ROLES         AGE       VERSION           INTERNAL-IP 
dmz-infra01           Ready     dmz-infra     18d       v1.11.0+d4cacc0   10.217.58.73
dmz-infra02           Ready     dmz-infra     18d       v1.11.0+d4cacc0   10.217.58.65
ktis-aidup-worker01   Ready     node          18d       v1.11.0+d4cacc0   10.217.69.50
ktis-aidup-worker02   Ready     node          18d       v1.11.0+d4cacc0   10.217.69.74
```

![3](쿠버7.png)

- Node 마다 고유의 IP를 부여
- Node의 IPTable에 의해 Node에서 동작하는 Pod로 패킹 라우팅이 이뤄짐



### Cluster 내부 통신

정상적으로 동작 중인 Pod 간에는 별도의 NetworkPolicy 정책을 적용하지 않았다면, 하나의 Pod 내부에서 다른 Pod로 통신이 가능합니다. NetworkPolicy 정책을 적용하면, 요청자의 IP나 목적지의 Port 정보 등을 설정하여 통신 여부를 제어할 수 있습니다.



#### Pod to Pod

```sh
NAMESPACE   NAME                                 READY     AGE       IP
devops      arsenal-assembler-7668df9c5c-5kpcf   Running   6d        192.168.10.134
okd-test    nginx2                               Running   12d       192.168.15.16
```



위와 같이 2개의 Namespace 공간에 각기 다른 Pod 2건이 존재할 때, "devops:arsenal-assembler-xxx" 내부에서 "okd-test:nginx2" Pod로 호출하는 예는 다음과 같이 정상적으로 확인 할 수 있습니다.



```sh
kubectl exec -it arsenal-assembler-7668df9c5c-5kpcf -- curl 192.168.15.16:80

<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
    body {
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>
```

> kubectl 또는 oc 를 사용 가능
>
> openshift 내부의 terminal 접속으로 하나의 Pod에 진입하여 목적지로 테스트 가능
>
> curl 또는 telnet, nmap 등 Pod의 Container에서 사용 가능한 Utility를 사용하여 점검 가능



#### Pod to Service

Pod 에서 Service 통신에는 Pod to Pod 와 동일한 방법으로 확인이 가능합니다. 아래와 같이 Service 구성이 2건 존재 하였을 때, Pod에서 다음과 같이 서비스 호출이 가능합니다.

```sh
NAME                  TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)     AGE
arsenal-assembler     ClusterIP   172.30.10.3      <none>        80/TCP      6d
arsenal-collector     ClusterIP   172.30.123.226   <none>        80/TCP      7d
kubectl exec -it arsenal-assembler-7668df9c5c-5kpcf -- curl arsenal-collector:80

<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
    body {
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>
```

> Pod 내부에서는 Service 명칭을 hostname으로 사용 가능하며, 직접적인 Service 명칭으로 명시 가능
>
> Service에 제공하는 Pod에서는 port로 명시된 Port에 접근하여야 하며, Service 설정된 targetPort로 통신이 이뤄짐



Service를 사용하면, 다수의 Pod가 존재하여도 하나의 단일 접점으로 제공 가능하게 되며, 논리적인 명칭으로 시스템 간에 안정적으로 사용 가능 하도록 제공됩니다. 다만, 서비스에 target에 해당하는 목적지 Pod의 정보 상태에 대해 확인이 필요한 경우 아래와 같이 endpoint를 확인 할 수 있습니다.

```sh
>kubectl get endpoints
NAME                                           ENDPOINTS              AGE
arsenal-assembler                              192.168.10.134:8080    6d
arsenal-collector                              192.168.10.127:8080    7d
```

> EndPoints는 Service 에서 목적지로 설정된 Pod 집합체 모두를 뜻함
>
> 다수의 Pod가 목적지인 경우 EndPoints "," 구분으로 포함
>
> EndPoints가 존재하지 않은 경우에 서비스 명칭으로 호출이 이뤄지면 불능이 발생



### Cluster 외부 통신

Cluster 외부와 통신하는 방법은 서비스 이용 측면에서 접속하는 최종 접점 사용자나 Cluster 외부의 시스템 간의 연동 등의 필요 상황 등에서 발생 될 수 있습니다. 따라서, 일반적으로 모든 서비스가 외부로 노출하지 않고, 필요한 서비스를 정의하여 해당 서비스만 Ingress (Openshift 기준 Route)를 통해 외부로 노출 시킬 수 있습니다. 다만, 이 방식에는 Http/Https 방식의 7계층 만 제공 가능하기에 TCP, UDP 등 3, 4계층 방식이 필요한 경우 Node IP를 통해 NodePort를 활용한 방법도 있습니다.

Arsenal Portal 을 통해 생성한 경우 Expose URL 설정으로 이와 같은 7계층 노출을 위한 제공이 되어 있습니다.


> Arsenal Portal 을 통해 "노출할 도메인 명" 을 활성화한 경우 "test-project" 명칭의 Service가 생성되며, Service 명칭은 도메인 명칭으로 노출되어, 해당 도메인 명칭으로 외부 접속이 가능



#### Inbound - Ingress(Router)

외부에서 접속 가능한 접점을 직접 만드는 방법은 Accordian의 경우에는 Ingress, OpenShift에서는 Router로 생성되어 제공하고 있습니다. 각각의 내용은 동일한 내용으로 구성되어 있으나, Ingress는 다수 건이 작성 가능하고, Router는 1건에 대해서만 작성 가능합니다. 다음은 Router를 사용한 예시입니다.



##### Ingress(Router) 정책 생성

```yaml
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  labels:
    app.kubernetes.io/instance: arsenal-assembler
    app.kubernetes.io/managed-by: Tiller
    app.kubernetes.io/name: arsenal-assembler
    app.kubernetes.io/version: "1.0"
    helm.sh/chart: arsenal-assembler-1.0.0
  name: arsenal-assembler-bckmp
  namespace: devops
spec:
  host: arsenal-assembler.c01-okd.cz-tb.paas.kt.co.kr
  path: /
  port:
    targetPort: http
  to:
    kind: Service
    name: arsenal-assembler
    weight: 100
  wildcardPolicy: None
```

- host : 노출될 명칭을 정의한다. Virtual Hostring 기능을 수행

- path : 라우팅 정책을 설정

- targetPort : 목적지 Service 접점의 Port 명칭

- to.name : 목적지 Service 명칭. Route가 존재하는 동일 Namespace의 서비스

- Default 설정으로 Router는 대상 서비스의 Pod를 sticky session방식으로 연결

- roundrobin 방식으로 변경하기 위해서는 아래와 같이 annotations을 추가

  ```yaml
  metadata:
    annotations:
      haproxy.router.openshift.io/balance: roundrobin
      haproxy.router.openshift.io/disable_cookies: "true"
  ```



##### 도메인 등록

Host 정보를 기준으로 DNS 시스템에 등록되었다면, 별도의 과정 없이 바로 외부 접점에서 호출이 가능합니다. Arsenal Portal에서 제공하는 기본 도메인 명칭을 사용하여 생성하였다면, Sub Domain에 대해서는 해당 Cluster 환경으로 기본 제공하고 있습니다. 그 외에 도메인 정보로 서비스를 수행해야 하면, 등록할 도메인 정보를 kate-WORK-BPM-신청 업무-도메인 통합 관리 시스템 http://ktdms.kt.com [도메인 신청] 등록합니다.

참고로 DNS에 등록되기 전이라면 로컬 hosts 파일에 도메인 정보를 각 Cluster 환경의 Infra Node 정보로 추가하여 테스트를 진행합니다. (Cluster 환경 별 Infra Node 정보는 별도 확인 필요)



##### 공인 인증서 신청

등록한 도메인 정보에 맞추어 공인 인증서 신청은 kate-WORK-BPM-신청 업무-도메인 통합 관리 시스템 http://ktdms.kt.com [SSL관리] 에서 가능합니다. TEST 및 특정 환경에 국한하여 자기 서명 인증서를 생성하여 테스트 가능합니다.



##### 인증서 설정

발급된 인증서를 Router를 기준으로 Https 전송 구간 암호화 설정하는 방법이며, 공인 인증서 발급 전까지 사설 인증서를 사용한 방법을 안내하고, 이후 정식 인증서가 발급되었을 때 적용하는 방법을 안내합니다.



###### 사설 인증서 생성 절차

인증서를 발행하는 과정이며, 별도의 공인 인증을 받지 않는 다면, 사설 인증서로 사용되는 형태입니다.

```sh
# 1. 개인키 생성
openssl genrsa -des3 -out micro-svc.key 1024
=> ex. 패스워드 입력: 123456

# 2. 패스워드 삭제 (현재 oc HAproxy는 password protected key files are not supported!!!!)
openssl rsa -in micro-svc.key -out micro-svc.key

# 3. 개인키 보기
openssl rsa -noout -text -in micro-svc.key

# 4. 인증서 생성
openssl req -new -x509 -sha1 -key micro-svc.key -out micro-svc.crt -days 7300
=> 이어서 나오는 몇 가지 질문에 대한 답변 예시
----
# Country Name (2 letter code) [XX]:KR
# State or Province Name (full name) []:SEOUL
# Locality Name (eg, city) [Default City]:SEOUL
# Organization Name (eg, company) [Default Company Ltd]:KT
# Organizational Unit Name (eg, section) []:KT
# Common Name (eg, your name or your server's hostname) []:static-demo.io
# Email Address []:email@kt.com

# 5. 인증서 보기
openssl x509 -noout -text -in micro-svc.crt
```



###### 공인 인증서

공인 인증서를 도메인 통합 관리 시스템에서 발급되면 아래와 같은 파일 목록을 제공 받습니다. 인증서 내부에 패스워드가 포함된 경우의 인증서를 사용하는 경우 서비스 동작 시에 비밀번호를 입력해야 하는 번거로움을 해제하기 위한 부분의 조치도 포함합니다.

```sh
-rwxr--r--. 1 root root 1988 Mar 11 19:22 ChainCA1.crt
-rwxr--r--. 1 root root 2210 Mar 11 19:22 ChainCA2.crt
-rwxr--r--. 1 root root 5744 Mar 11 19:22 Chain_RootCA_Bundle.crt
-rwxr--r--. 1 root root 1546 Mar 11 19:22 RootCA.crt
-rwxr--r--. 1 root root 2548 Mar 11 19:22 test_insuretech_kt_com_cert.pem
-rwxr--r--. 1 root root 1679 Mar 11 19:25 test_insuretech_kt_com_key.pem

# 패스워드 삭제 (현재 oc HAproxy는 password protected key files are not supported!!!!)
openssl rsa -in test_insuretech_kt_com_key.pem -out test_insuretech_kt_com_key.pem
```



###### Router에 인증서 적용

```yaml
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: route-athn
  labels:
    appName: mmp
    svcName: athn
    version: v1
spec:
  host: test.insuretech.kt.com
  path: /athn
  port:
    targetPort: http
  tls:
    caCertificate: |-
      -----BEGIN CERTIFICATE-----
      MIIGGTCCBAGgAwIBAgIQE31TnKp8MamkM3AZaIR6jTANBgkqhkiG9w0BAQwFADCB
      iDELMAkGA1UEBhMCVVMxEzARBgNVBAgTCk5ldyBKZXJzZXkxFDASBgNVBAcTC0pl
      (생략)...
      idWkvklsQLI+qGu41SWyxP7x09fn1txDAXYw+zuLXfdKiXyaNb78yvBXAfCNP6CH
      MntHWpdLgtJmwsQt6j8k9Kf5qLnjatkYYaA7jBU=
      -----END CERTIFICATE-----
      -----BEGIN CERTIFICATE-----
      MIIFdzCCBF+gAwIBAgIQE+oocFv07O0MNmMJgGFDNjANBgkqhkiG9w0BAQwFADBv
      MQswCQYDVQQGEwJTRTEUMBIGA1UEChMLQWRkVHJ1c3QgQUIxJjAkBgNVBAsTHUFk
      (생략)...
      Jtl7GQVoP7o81DgGotPmjw7jtHFtQELFhLRAlSv0ZaBIefYdgWOWnU914Ph85I6p
      0fKtirOMxyHNwu8=
      -----END CERTIFICATE-----
      -----BEGIN CERTIFICATE-----
      MIIENjCCAx6gAwIBAgIBATANBgkqhkiG9w0BAQUFADBvMQswCQYDVQQGEwJTRTEU
      MBIGA1UEChMLQWRkVHJ1c3QgQUIxJjAkBgNVBAsTHUFkZFRydXN0IEV4dGVybmFs
      (생략)...
      c4g/VhsxOBi0cQ+azcgOno4uG+GMmIPLHzHxREzGBHNJdmAPx/i9F4BrLunMTA5a
      mnkPIAou1Z5jJh5VkpTYghdae9C8x49OhgQ=
      -----END CERTIFICATE-----
    certificate: |-
      -----BEGIN CERTIFICATE-----
      MIIHLjCCBhagAwIBAgIQOO7kgEA1TXra8OnlU3YfLTANBgkqhkiG9w0BAQsFADCB
      lTELMAkGA1UEBhMCR0IxGzAZBgNVBAgTEkdyZWF0ZXIgTWFuY2hlc3RlcjEQMA4G
      (생략)...
      VNsmE8pLfpvMziQmhvFB7D5xOJwRRU95uBCj7tfnCJ10jXsLlMQvmbY5+5VLSVs0
      uRT+Xqz+4ibdSteU61G3jyrt
      -----END CERTIFICATE-----
    insecureEdgeTerminationPolicy: Redirect
    key: |-
      -----BEGIN RSA PRIVATE KEY-----
      MIIEpAIBAAKCAQEAhrCgZR2fuexleY1qnGgRFBSZeLYdnNzVElPAQ72JH/icKjd0
      pFWx/I6QKFrQYEFebGmoXQFYTL0nJhOb+276aL6iTqzPFr05Y0PH1pWU0rTD+B85
      (생략)...
      xOhzrdKZ7USQvFh2Pu/592hRevpyxmQ0Qvim6+0+SbHZdmydNVtb+3ol2ZLggJNx
      IohfEwqalSH2gPMNSX4UmQWr6/eXJvJ5rbwQwrqencJ7WCph1v+/lw==
      -----END RSA PRIVATE KEY-----
    termination: edge
  to:
    kind: Service
    name: service-athn
    weight: 100
  wildcardPolicy: None
```

> 개인키(key), 인증서(certificate)와 더불어 chain+root ca 인증서(caCertificate)도 함께 설정
>
> 그 외 TLS 설정에 관련된 항목 설명
>
> ​	spec.insecureEdgeTerminationPolicy
>
> ```
>   - None : HTTP 허용하지 않음
>   - Allow : HTTP 허용
>   - Redirect : HTTP에 대해 Redirect 302 Status로 리턴
> ```
>
> ​    spec.termination
>
> - edge : TLS(암호화) 종단점이 되어 이후 HTTP로 목적지로 명시한 Service로 전달
> - passthrough : 모든 네트워크를 그대로 목적지로 통과
> - re-encryption : TLS 전송 구간을 종단점으로 설정하고, 이후 새로운 키와 인증서로 전송 구간을 생성



#### Outbound

Cluster 환경에서 외부로 호출하는 방법에는 크게 두 가지가 존재합니다.  첫 번째는 일반적으로 Pod에서 출발해서 외부 목적지로 출발하는 트래픽은 Pod가 동작 중인 Node IP를 기준으로 발생됩니다. 이 경우에는 방화벽 출발지를 Node IP를 설정하여 가능합니다. Pod가 운용될 Node IP가 다수라면, 출발지 설정에 모두 포함해야 합니다.

두 번째는 Static IP를 할당 받아, Egress를 설정하여 Egress Controller, Gateway, Router Pods를 거쳐 접근하는 방법입니다. 출발지 IP를 단일화 할 수 있습니다.



##### 개요

Egress는 타깃 서버로 나가는 Outgoing 트래픽 전송을 담당하고 그 때 특정 소스 IP  주소를 가집니다. 애플리케이션은 외부 실제 서비스를 직접 호출하는 것이 아니라 Egress Router 서비스를 호출하게 되고 이는 Egress Router Pod에 의해 재전달 됩니다. Egress Router가 모든 Outgoing 트래픽을 위해 사용하는 것은 권고하지 않습니다.  많은 Egress Router의 생성은 네트워크 리소스의 제약을 가져옵니다. Egress Router는 Primary Network Interface에 IP를 추가하는 하는 방식입니다. 이때의  IP는 노드 IP와 동일한  Subnet을 가져야 합니다. Egress Router Pod에는 macvlan interface로 해당 IP가 설정됩니다.



##### 사전 준비

클러스터 관리자는 노드 별 사용 가능한  static IP를 할당합니다.

```sh
$ oc patch hostsubnet <node_name> -p \
'{"egressIPs": ["<IP_address_1>", "<IP_address_2>"]}'
$ oc patch hostsubnet herasoo-dmz-infra-wokernode1 -p \
  '{"egressIPs": ["192.168.12.99", "192.168.12.100", "192.168.12.101"]}'
```



클러스터 관리자는 Namespace 별 사용 가능한 Static IP를 할당합니다. 이는 특정 Namespace에서 나가는 트래픽을 쉽게 구분 시켜 주는 장점이 있습니다.

```sh
$ oc patch netnamespace <project_name> -p '{"egressIPs": ["<IP_address>"]}'
$ oc patch netnamespace herasoo -p '{"egressIPs": ["192.168.12.99"]}'
```



위와 같이 설정한다면 herasoo Namespace에 생성되는 Egress Router Pod는 herasoo-dmz-infra-wokernode1 Node에 배치되고 그 때  static IP 192.168.12.99를 가지게 된다. 이로써 Node 별 static IPs, Namespace별 static IPs  관리되어 효율적인 관리와 가시적인 모니터링이 가능합니다.



##### Sample Router

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deployment-egress-route-1
spec:
  selector:
    matchLabels:
      app: egress-route-1
  replicas: 1
  template:
    metadata:
      labels:
        app: egress-route-1
      annotations:
        pod.network.openshift.io/assign-macvlan: "true"
    spec:
      initContainers:
      - name: egress-router
        image: ktis-bastion01.container.ipc.kt.com:5000/openshift3/ose-egress-router:v3.11
        imagePullPolicy: Always
        env:
        - name: EGRESS_SOURCE
          value: 192.168.12.99
        - name: EGRESS_GATEWAY
          value: 192.168.12.1
        - name: EGRESS_DESTINATION
          #value: 203.0.113.25
          value: |
            80   tcp 203.0.113.25
            443	 tcp 203.0.113.25	
            8080 tcp 203.0.113.26 80
            8443 tcp 203.0.113.26 443
            #203.0.113.27
        - name: EGRESS_ROUTER_MODE
          value: init
        securityContext:
          privileged: true
      containers:
      - name: egress-router-wait
        image: ktis-bastion01.container.ipc.kt.com:5000/openshift3/ose-pod:v3.11
      #nodeSelector:
        #site: springfield-1
      nodeName: herasoo-dmz-infra-wokernode1
      restartPolicy: Always
```

- replicas: 1` 1이어야만 해당 소스IP 192.168.12.99를 가질 수 있음

- nodeName: herasoo-dmz-infra-wokernode1 해당 노드에 192.168.12.99 static IP가 할당 되어 있음

- name: EGRESS_SOURCE value: 192.168.12.99 Router의 소스 IP

- name: EGRESS_GATEWAY value: 192.168.12.1 herasoo-dmz-infra-wokernode1 의 GW IP정보와 같아야 함

- name: EGRESS_DESTINATION

  - Redirect Traffic Rule을 작성

    ```sh
    80   tcp 203.0.113.25	  # Router에 80으로 들어오면 tcp 203.0.113.25 80으로 보낸다.
    443	 tcp 203.0.113.25	  # Router에 443으로 들어오면 tcp 203.0.113.25 443으로 보낸다.
    8080 tcp 203.0.113.26 80  # Router에 8080으로 들어오면 tcp 203.0.113.26 80으로 보낸다.
    8443 tcp 203.0.113.26 443 # Router에 8443으로 들어오면 tcp 203.0.113.26 443으로 보낸다.
    203.0.113.27			  # fallback IP로 위에 정해지지 않는 포트로 들어왔을 때 203.0.113.27의 포트로 보낸다. 설정되지 않았다면 Redirect가 거절된다.
    ```

- value: init initContainer 형태로 동작



##### Sample Router Service

```yaml
apiVersion: v1
kind: Service
metadata:
  name: service-egress-route-1
spec:
  ports:
  - name: http-remote1	 # 192.168.12.99를 소스IP로 가지고 203.0.113.25 80으로 전송된다.
    port: 80
  - name: https-remote1	 # 192.168.12.99를 소스IP로 가지고 203.0.113.25 443으로 전송된다.
    port: 443
  - name: http-remote2	 # 192.168.12.99를 소스IP로 가지고 203.0.113.26 80으로 전송된다.
    port: 8080
  - name: https-remote2	 # 192.168.12.99를 소스IP로 가지고 203.0.113.26 443으로 전송된다.
    port: 8443
  type: ClusterIP
  selector:
    app: egress-route-1
```





## 쿠버네티스 리소스 관리 가이드

### kt OpenShift 환경 주요 리소스 목록

kt OpenShift 환경의 주요 리소스 목록은 아래와 같다. 전체 목록은 해당 가이드 하단에 첨부된 "[부록] 쿠버네티스 리소스 전체 목록 및 약어" 부분을 참고하기 바란다.

- Namesapce
- Volume
- Services
- Service Accounts
- Role & Cluster Roles
- ConfigMaps
- Deployments
- CronJob

#### Namespace 관리 가이드

Namespace (OpenShift 환경에서는 `Project` 라 부름) 는 시스템 단위로 할당되며, 직접적인 생성은 kt 정책 상 불가하고, Cloud아키텍트 조직(가칭)에 요청하여 할당 받는다.

#### Service Accounts 관리 가이드

쿠버네티스는 API 서버에 접속하는 두 가지 유형의 클라이언트를 구분한다.

| Type | Subject                                            | Mechanism       |
| ---- | -------------------------------------------------- | --------------- |
| 휴먼 | 사용자                                             | OpenShift 계정  |
| 포드 | 더 구체적으로, 포드 내부에서 실행하는 어플리케이션 | Service Account |

이 중 포드에서는 Service Accounts (SA) 라는 매커니즘을 사용한다.

모든 포드는 포드에서 실행 중인 어플리케이션을 식별할 수 있는 SA와 연관된다. SA는 Kubernetes 리소스이며 그 말은 즉슨 API 서버를 통해 생성, 업데이트 또는 삭제가 가능하다는 의미와 같다.

SA를 별도 생성을 하지 않으면 포드는 프로젝트 생성 시 같이 생성되는 `default` SA를 사용하게 된다. default SA는 클러스터 상태를 볼 수 없으며 추가 권한을 부여하지 않는 한 어떤 식으로든 클러스터를 수정할 수 없다.

이런 동작 원리를 가지게 된 배경에는 클러스터 보안이 있다.

클러스터 메타 데이터를 읽을 필요가 없는 포드는 클러스터에 배포된 리소스를 검색하거나 수정할 수 없는 SA에서 실행해야 한다.

반면, 리소스 메타 데이터를 검색해야 하는 포드는 해당 객체의 메타 데이터를 읽기 가능한 SA에서 실행해야 하고, 메타 데이터 수정이 필요한 포드는 수정 권한이 있는 SA에서 실행해야 한다.

##### kt Service Accounts 정책

아래에 해당하는 경우가 아니라면 default SA를 사용한다.

- 어플리케이션에서 쿠버네티스 API 서버와 통신이 필요한 경우
- 어플리케이션에서 쿠버네티스 메타 데이터 조회가 필요한 경우
- 어플리케이션에서 쿠버네티스 메타 데이터 수정이 필요한 경우

SA 생성 후 kt 지정 롤 중 하나를 바인딩한다.

- admin / edit / view

##### SA 생성 방법

```sh
# Service Account 생성
$ oc create sa foo
serviceaccount "foo" create

# Role Binding 수행
$ cat > xxx-rolebinding.yaml
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
name: xxx_rolebinding
namespace: millet
subjects:
- kind: ServiceAccount
name: custom-sa
namespace: millet
roleRef:
apiGroup: rbac.authorization.k8s.io
kind: ClusterRole
name: admin	# kt 롤 관리 정책 중 admin 롤을 적용한 예시
---
$ oc create -f tiller_rolebinding.yaml
rolebinding "tiller_rolebinding" create
```

#### Role & ClusterRole 관리 가이드

kt 롤 관리 정책에 의거해 admin, edit, view 3개의 롤을 이용한다.

자세한 사항은 "사용자 관리 가이드" 내 [kt 롤 관리 정책](http://gitlab.msa.kt.com/coe-istio-master/msa-bunker/blob/master/deliverables/12.%20%EC%82%AC%EC%9A%A9%EC%9E%90%20%EB%B0%8F%20%EB%A6%AC%EC%86%8C%EC%8A%A4%20%EA%B4%80%EB%A6%AC%20%EA%B0%80%EC%9D%B4%EB%93%9C/%EC%82%AC%EC%9A%A9%EC%9E%90%20%EA%B4%80%EB%A6%AC%20%EA%B0%80%EC%9D%B4%EB%93%9C.md#3-kt-%EB%A1%A4-%EA%B4%80%EB%A6%AC-%EC%A0%95%EC%B1%85) 을 참고한다.

#### Volume 가이드

쿠버네티스 포드는 내부에서 실행되는 프로세스가 CPU, RAM, 네트워크 인터페이스 등의 리소스를 공유하는 논리적 호스트와 유사하다. 그 과정에서 디스크 역시 공유할 것이라 생각할 수 있지만, 사실은 그렇지 않다. 파일 시스템은 컨테이너의 이미지에서 제공되기 때문에 포드의 각 컨테이너는 고유의 분리된 파일 시스템이 있다.

컨테이너는 빌드할 때 이미지에 추가된 파일 세트로 시작한다. 이전 컨테이너가 죽고 새로운 컨테이너가 구동될 때, 이전 컨테이너가 파일 시스템에 기록한 내용은 새 컨테이너에 표시되지 않는다.

컨테이너가 어느 노드에서 뜨던, 죽었다 새로 구동됐던 실제 데이터가 있는 디렉토리를 보존하고 싶을 경우 볼륨을 정의하여 사용한다.

쿠버네티스의 볼륨은 포드의 컴포넌트이므로 컨테이너와 마찬가지로 포드의 스펙에 정의되고 포드의 라이프사이클과 동일하게 움직인다. 이는 포드가 시작될 때 볼륨이 작성되고 포드가 삭제될 때 볼륨이 삭제됨을 의미한다.

컨테이너와 볼륨이 동일한 포드에 있다 해도 컨테이너는 해당 파일에 액세스할 수 없으며 볼륨 마운트를 컨테이너 스펙에 정의해야 비로소 액세스가 가능하다.

볼륨 타입을 3가지로 분류하면 아래와 같다.

| temp     | local    | network                                                      |
| -------- | -------- | ------------------------------------------------------------ |
| emptyDir | hostPath | nfs<br />gitRepo<br />iSCSI<br />gcePersistentDisk<br />etc. |

##### emptyDir 볼륨

###### 소개 및 정의

> 동일한 포드에서 실행 중인 컨테이너 간에 파일 공유가 필요할 경우 emptyDir을 사용한다.

emptyDir은 포드가 생성될 때 생성되고, 포드가 삭제 될 때 같이 삭제되는 임시 볼륨이다. 포드가 생성될 때 아무 내용이 없기 때문에 emptyDir이라고 한다.

쿠버네티스에서 볼륨이란 포드에 종속되는 디스크이다. 컨테이너 단위가 아닌 포드 단위이기 때문에, 그 포드에 속해 있는 여러 개의 컨테이너가 공유해서 사용할 수 있다.

###### 사용방법

아래 yaml 은 간단한 emptyDir 볼륨의 예시이다.

Pod 내 2개의 컨테이너가 있고, 첫 번째 컨테이너 html-generator에 의해 생성된 index.html 파일을 두 번째 컨테이너인 nginx가 서비스하는 프로그램이다.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-volume-test
spec:
  containers:
    - image: emoket/html-generator:1.0
      name: html-generator
      volumeMounts:
        - name: html
          mountPath: /usr/share/nginx/html
    - image: nginx:alpine
      name: web-server
      volumeMounts:
        - name: html
          mountPath: /usr/share/nginx/html
          readOnly: true
      ports:
        - containerPort: 80
          protocol: TCP
  volumes:
    - name: html
      emptyDir: {}
      #  medium: Memory
```

- 볼륨으로 사용한 emptyDir 볼륨은 포드를 호스팅하는 워커노드의 실제 디스크에 생성됐으므로 노드 디스크 유형에 따라 성능이 달라진다. 위 예시 마지막에 `medium: Memory` 로 설정해 디스크 대신 메모리에 있는 tmpfs 파일 시스템을 이용할 수도 있다.

##### hostPath 볼륨

###### 소개 및 정의

> 워커노드 파일 시스템에 액세스가 필요할 경우 hostPath를 사용한다.

hostPath 볼륨은 영구 스토리지 유형의 한 종류로 포드가 죽으면 삭제되는 emptyDir과 다르게 포드가 죽어도 삭제되지 않는다.

> 단, 이러한 정책이 노드 레벨에서 이루어지기 때문에 포드가 다른 노드에서 기동될 경우, 이전 노드에서 사용한 hostPath의 파일 내용에는 엑세스가 불가능하다.

![hostPath](볼륨.PNG)

###### 사용방법

아래는 노드의 /tmp 디렉토리를 hostPath 볼륨을 이용하여 /data/shared 디렉토리에 마운트 하여 사용하는 예제이다.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: volume-hostpath-simple
spec:
  containers:
    - name: my-redis
      image: emoket/redis
      volumeMounts:
        - name: my-redis-path
          mountPath: /data/shared
  volumes:
    - name: my-redis-path
      hostPath:
        path: /tmp
        type: Directory
```

##### nfs 볼륨

###### 소개 및 정의

> 모든 클러스터 노드에서 액세스가 필요할 경우 nfs 볼륨을 사용한다.

포드에서 실행 중인 애플리케이션이 디스크에 데이터를 유지해야 하고 포드가 다른 노드로 다시 예약된 경우에도 동일한 데이터를 사용할 수 있어야 하는 경우 위에서 언급한 emptyDir, hostPath 볼륨을 사용할 수 없다. 모든 클러스터 노드에서 액세스할 수 있어야 하므로 NAS 유형에 저장해야 한다.

GCE, AWS 등 다양한 옵션이 존재하나 kt의 경우 nfs를 사용한다.

> NFS stands for Network File System – it's a shared filesystem that can be accessed over the network.

쿠버네티스가 NFS를 운영하지 않고 포드가 액세스 하는 구조이기 때문에 nfs 볼륨 사용 전 미리 nfs 서버를 구성해 둬야 한다.

###### 사용방법

```yaml
# Create a pod that reads and writes to the
# NFS server via an NFS volume.

kind: Pod
apiVersion: v1
metadata:
  name: pod-using-nfs
spec:
  # Add the server as an NFS volume for the pod
  volumes:
    - name: nfs-volume
      nfs:
        # URL for the NFS server
        server: 10.108.211.244 # Change this!
        path: /

  # In this container, we'll mount the NFS volume
  # and write the date to a file inside it.
  containers:
    - name: app
      image: alpine

      # Mount the NFS volume in the container
      volumeMounts:
        - name: nfs-volume
          mountPath: /var/nfs

      # Write to a file inside our NFS
      command: ['/bin/sh']
      args: ['-c', 'while true; do date >> /var/nfs/dates.txt; sleep 5; done']
```



#### Services 가이드

쿠버네티스에서는 포드가 새로 기동될 때마다 IP주소와 Port가 변하기 때문에 고정된 엔드포인트로 호출을 하기가 어렵다. 또한 여러 포드에 같은 애플리케이션을 운용할 경우 이 포드 간의 로드밸런싱을 지원해 줘야 하는데 이러한 역할을 하는 리소스가 서비스이다.

쿠버네티스 서비스는 동일한 서비스를 제공하는 포드 그룹에 단일 진입점을 만들어주고 서비스가 존재하는 동안 절대로 변경되지 않는 IP주소와 Port가 생기게 된다. 또한 여러 포드를 묶어서 로드밸런싱 처리가 가능하고 고유한 DNS 이름을 가질 수 있다.

##### Cluster IP 서비스

###### 소개 및 정의

> 클러스터 안에서만 유효한 서비스 호출일 경우 Cluster IP 타입을 사용한다.

![Kubernetes Service](쿠버8.PNG)

###### 사용 방법

아래는 80과 443 포트로 들어오는 연결을 허용하고 `app=webapp` 라벨셀렉터에 매칭되는 포드들로 연결시키는 예시이다.

```yaml
apiVersion: v1
kind: Service
metadata:
  name: simple01-web
  labels:
    app: simple01-web
    version: 1.0.0
    chart: simple01-1.0.0
    release: simple01
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app: simple01-web
    version: 1.0.0
    chart: simple01-1.0.0
    release: simple01
```

[ 서비스의 클러스터 IP 확인 ]

```sh
$ kubectl get svc
NAME         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)           AGE
service-foo  ClusterIP   10.105.245.138   <none>        80/TCP, 443/TCP   4d
```

- 서비스로 할당된 IP주소가 10.105.245.138 임을 알 수 있다. 이 주소는 클러스터의 IP주소이기 때문에 클러스터 안에서만 유효하다.



##### Ingress

###### 소개 및 정의

클러스터 외부에서 내부로 접근하는 요청들을 어떻게 처리할지 정의해둔 규칙들의 모음을 Ingress라 하고 실제 규칙에 맞는 동작을 수행하는 주체를 Ingress Controller라 한다.

**[ Ingress 제공 기능 ]**

- 외부에서 접근 가능한 URL을 사용할 수 있게 한다.
- 트래픽 로드밸런싱 지원
- SSL 인증서 처리
- 도메인 기반 가상 호스팅 제공

###### 사용방법

```yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: waxen-ibex-simple02
  labels:
    app: simple02
    version: 1.0.0
    chart: simple02-1.0.0
    release: waxen-ibex
spec:
  rules:
    - host: simple02-package.container.ipc.kt.com
      http:
        paths:
          - path: /
            backend:
              serviceName: simple02-web
              servicePort: http
```



#### configmaps 사용 가이드

##### configmaps

###### 소개 및 정의

컨테이너 이미지에서 사용하는 환경변수와 같은 세부 정보를 별도로 분리해서 사용하고자 할 때 configmap을 사용한다.



###### 사용방법

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: simple01-redis-config
data:
  redis-config: |
    maxmemory 128mb
    maxmemory-policy allkeys-lru
```



#### deployments 사용 가이드

##### deployments

###### 소개 및 정의

포드를 띄우는 주체로 deployment 오브젝트를 정의한다. 실제 구동은 deployment > replicaset > pod 순으로 생성된다.

deployment 정의 시 labels 항목은 아래와 같은 포맷을 유지하도록 한다.

```yaml
app: simple01-webversion: 1.0.0chart: simple01-1.0.0release: simple01
```

###### 사용방법

```yaml
apiVersion: apps/v1beta2kind: Deploymentmetadata:  name: simple01-web-1.0.0  labels:    app: simple01-web    version: 1.0.0    chart: simple01-1.0.0    release: simple01spec:  replicas: 1  selector:    matchLabels:      app: simple01-web      version: 1.0.0      chart: simple01-1.0.0      release: simple01  template:    metadata:      labels:        app: simple01-web        version: 1.0.0        chart: simple01-1.0.0        release: simple01    spec:      containers:        - name: web          image: 'ktis-bastion01.container.ipc.kt.com:5000/millet/web-sample:1.0.0'          imagePullPolicy: Always          env:            - name: spring.profiles.active              value: 'dev'            - name: target.was.service.name              value: 'simple01-was'          ports:            - containerPort: 8080              name: http              protocol: TCP          livenessProbe:            httpGet:              path: /actuator/health              port: http          readinessProbe:            httpGet:              path: /actuator/health              port: http          resources: {}      nodeSelector: {}      affinity: {}      tolerations: []
```



#### cronjobs 사용 가이드

##### cronjobs

###### 소개 및 정의

cronjob은 job을 시간 기준으로 관리한다. 지정된 시간에 한 번 혹은 주기적으로 job을 실행하며, 복수개의 job을 실행할 수도 있다.

cronjob은 job을 생성하는 역할만 하고 실제 동작은 job에서 수행된다.

###### 사용방법

```yaml
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: intent-bunny-batch
  labels:
    app: intent-bunny
    version: 1.0.0
    chart: simple05-1.0.0
    release: intent-bunny
spec:
  schedule: '*/5 * * * *'	# crontab
  failedJobsHistoryLimit: 3
  concurrencyPolicy: Allow
  jobTemplate:
    metadata:
      labels:
        appName: intent-bunny
        svcName: intent-bunny-batch
        version: 1.0.0
        chart: simple05-1.0.0
        release: intent-bunny
    spec:
      template:
        metadata:
          labels:
            appName: intent-bunny
            svcName: intent-bunny-batch
            version: 1.0.0
            chart: simple05-1.0.0
            release: intent-bunny
        spec:
          containers:
            - name: batch
              image: 'ktis-bastion01.container.ipc.kt.com:5000/millet/batch-sample:1.0.0'
              imagePullPolicy: Always
              args: [--job.name=simpleJob]
              env:
                - name: POSTGRES_ENV_DB
                  value: 'helm_sample'
                - name: POSTGRES_ENV_IP
                  value: 'simple05-postgresql'
                - name: POSTGRES_ENV_PORT
                  value: '5432'
                - name: spring.profiles.active
                  value: 'dev'
          restartPolicy: Never
```




### [부록] 쿠버네티스 리소스 전체 목록 및 약어

| API GROUPS                 | RESOURCES                           | ABBREBIATION |
| -------------------------- | ----------------------------------- | ------------ |
| ""                         | configmaps                          | CM           |
|                            | endpoints                           | E            |
|                            | persistentvolumeclaims              | PVC          |
|                            | pods                                | P            |
|                            | pods/log                            | P            |
|                            | pods/status                         | P            |
|                            | pods/attach                         | P            |
|                            | pods/exec                           | P            |
|                            | pods/portforward                    | P            |
|                            | pods/proxy                          | P            |
|                            | replicationcontrollers              | RC           |
|                            | replicationcontrollers/scale        | RC           |
|                            | replicationcontrollers/status       | RC           |
|                            | serviceaccounts                     | SA           |
|                            | secrets                             | S            |
|                            | services                            | S            |
|                            | services/proxy                      | S            |
|                            | bindings                            | B            |
|                            | events                              | E            |
|                            | limitranges                         | LR           |
|                            | resourcequotas                      | RQ           |
|                            | resourcequotas/status               | RQ           |
|                            | namespaces                          | NS           |
|                            | namespaces/status                   | NS           |
|                            | resourcequotausages                 | RES          |
| apps                       | daemonsets                          | DS           |
|                            | deployments                         | D            |
|                            | deployments/scale                   | D            |
|                            | deployments/rollback                | D            |
|                            | replicasets                         | RS           |
|                            | replicasets/scale                   | RS           |
|                            | statefulsets                        | SS           |
|                            | statefulsets/scale                  | SS           |
| autoscaling                | horizontalpodautoscalers            | HPA          |
| batch                      | cronjobs                            | CJ           |
|                            | jobs                                | J            |
| extensions                 | daemonsets                          | DS           |
|                            | deployments                         | D            |
|                            | deployments/rollback                | D            |
|                            | deployments/scale                   | D            |
|                            | ingresses                           | I            |
|                            | networkpolicies                     | NP           |
|                            | replicasets                         | RS           |
|                            | replicasets/scale                   | RS           |
|                            | replicationcontrollers/scale        | RC           |
| policy                     | poddisruptionbudgets                | PDB          |
| networking.k8s.io          | networkpolicies                     | NP           |
| authorization.k8s.io       | localsubjectaccessreviews           | LSA          |
| rbac.authorization.k8s.io  | roles                               | R            |
|                            | rolebindings                        | RB           |
| authorization.openshift.io | roles                               | R            |
|                            | rolebindings                        | RB           |
|                            | rolebindingrestrictions             | RBR          |
|                            | localresourceaccessreviews          | LRA          |
|                            | localsubjectaccessreviews           | LSA          |
|                            | subjectrulesreviews                 | SRR          |
|                            | resourceaccessreviews               | RAR          |
|                            | subjectaccessreviews                | SAR          |
| security.openshift.io      | podsecuritypolicyreviews            | PSP          |
|                            | podsecuritypolicyselfsubjectreviews | PSP          |
|                            | podsecuritypolicysubjectreviews     | PSP          |
| build.openshift.io         | buildconfigs                        | BC           |
|                            | buildconfigs/webhooks               | BC           |
|                            | builds                              | B            |
|                            | builds/log                          | B            |
|                            | builds/clone                        | B            |
|                            | builds/details                      | B            |
|                            | buildconfigs/instantiate            | BC           |
|                            | buildconfigs/instantiatebinary      | BC           |
|                            | buildlogs                           | BUI          |
|                            | jenkins                             | JEN          |
| apps.openshift.io          | deploymentconfigs                   | DC           |
|                            | deploymentconfigs/scale             | DC           |
|                            | deploymentconfigrollbacks           | DEP          |
|                            | deploymentconfigs/instantiate       | DC           |
|                            | deploymentconfigs/rollback          | DC           |
|                            | deploymentconfigs/log               | DC           |
|                            | deploymentconfigs/status            | DC           |
| image.openshift.io         | imagestreamimages                   | ISI          |
|                            | imagestreammappings                 | ISM          |
|                            | imagestreams                        | IS           |
|                            | imagestreams/secrets                | IS           |
|                            | imagestreams/status                 | IS           |
|                            | imagestreams/layers                 | IS           |
|                            | imagestreamtags                     | IST          |
|                            | imagestreamimports                  | ISI          |
| project.openshift.io       | projects                            | PR           |
| quota.openshift.io         | appliedclusterresourcequotas        | ACR          |
| route.openshift.io         | routes                              | RT           |
|                            | routes/custom-host                  | RT           |
|                            | routes/status                       | RT           |
| template.openshift.io      | processedtemplates                  | PRO          |
|                            | templateconfigs                     | TEM          |
|                            | templateinstances                   | TI           |
|                            | templates                           | T            |




#### kt 정책 3가지 권한 상세

kt OpenShift 환경에서 사용하는 view, edit, admin 클러스터롤의 리소스 권한 목록은 각각 아래와 같다.



##### view

```sh
$ kubectl describe clusterrole view
Name:         view
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
PolicyRule:
  Resources                                Non-Resource URLs  Resource Names  Verbs
  ---------                                -----------------  --------------  -----
  bindings                                 []                 []              [get list watch]
  configmaps                               []                 []              [get list watch]
  endpoints                                []                 []              [get list watch]
  events                                   []                 []              [get list watch]
  limitranges                              []                 []              [get list watch]
  namespaces/status                        []                 []              [get list watch]
  namespaces                               []                 []              [get list watch]
  persistentvolumeclaims                   []                 []              [get list watch]
  pods/log                                 []                 []              [get list watch]
  pods/status                              []                 []              [get list watch]
  pods                                     []                 []              [get list watch]
  replicationcontrollers/scale             []                 []              [get list watch]
  replicationcontrollers/status            []                 []              [get list watch]
  replicationcontrollers                   []                 []              [get list watch]
  resourcequotas/status                    []                 []              [get list watch]
  resourcequotas                           []                 []              [get list watch]
  serviceaccounts                          []                 []              [get list watch]
  services                                 []                 []              [get list watch]
  controllerrevisions.apps                 []                 []              [get list watch]
  daemonsets.apps                          []                 []              [get list watch]
  deployments.apps/scale                   []                 []              [get list watch]
  deployments.apps                         []                 []              [get list watch]
  replicasets.apps/scale                   []                 []              [get list watch]
  replicasets.apps                         []                 []              [get list watch]
  statefulsets.apps/scale                  []                 []              [get list watch]
  statefulsets.apps                        []                 []              [get list watch]
  horizontalpodautoscalers.autoscaling     []                 []              [get list watch]
  cronjobs.batch                           []                 []              [get list watch]
  jobs.batch                               []                 []              [get list watch]
  daemonsets.extensions                    []                 []              [get list watch]
  deployments.extensions/scale             []                 []              [get list watch]
  deployments.extensions                   []                 []              [get list watch]
  ingresses.extensions                     []                 []              [get list watch]
  networkpolicies.extensions               []                 []              [get list watch]
  replicasets.extensions/scale             []                 []              [get list watch]
  replicasets.extensions                   []                 []              [get list watch]
  replicationcontrollers.extensions/scale  []                 []              [get list watch]
  networkpolicies.networking.k8s.io        []                 []              [get list watch]
  poddisruptionbudgets.policy              []                 []              [get list watch]
```



##### edit

```sh
$ kubectl describe clusterrole edit
Name:         edit
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
PolicyRule:
  Resources                                Non-Resource URLs  Resource Names  Verbs
  ---------                                -----------------  --------------  -----
  serviceaccounts                          []                 []              [create delete deletecollection get list patch update watch impersonate]
  configmaps                               []                 []              [create delete deletecollection get list patch update watch]
  endpoints                                []                 []              [create delete deletecollection get list patch update watch]
  persistentvolumeclaims                   []                 []              [create delete deletecollection get list patch update watch]
  pods/attach                              []                 []              [create delete deletecollection get list patch update watch]
  pods/exec                                []                 []              [create delete deletecollection get list patch update watch]
  pods/portforward                         []                 []              [create delete deletecollection get list patch update watch]
  pods/proxy                               []                 []              [create delete deletecollection get list patch update watch]
  pods                                     []                 []              [create delete deletecollection get list patch update watch]
  replicationcontrollers/scale             []                 []              [create delete deletecollection get list patch update watch]
  replicationcontrollers                   []                 []              [create delete deletecollection get list patch update watch]
  secrets                                  []                 []              [create delete deletecollection get list patch update watch]
  services/proxy                           []                 []              [create delete deletecollection get list patch update watch]
  services                                 []                 []              [create delete deletecollection get list patch update watch]
  controllerrevisions.apps                 []                 []              [create delete deletecollection get list patch update watch]
  daemonsets.apps                          []                 []              [create delete deletecollection get list patch update watch]
  deployments.apps/rollback                []                 []              [create delete deletecollection get list patch update watch]
  deployments.apps/scale                   []                 []              [create delete deletecollection get list patch update watch]
  deployments.apps                         []                 []              [create delete deletecollection get list patch update watch]
  replicasets.apps/scale                   []                 []              [create delete deletecollection get list patch update watch]
  replicasets.apps                         []                 []              [create delete deletecollection get list patch update watch]
  statefulsets.apps/scale                  []                 []              [create delete deletecollection get list patch update watch]
  statefulsets.apps                        []                 []              [create delete deletecollection get list patch update watch]
  horizontalpodautoscalers.autoscaling     []                 []              [create delete deletecollection get list patch update watch]
  cronjobs.batch                           []                 []              [create delete deletecollection get list patch update watch]
  jobs.batch                               []                 []              [create delete deletecollection get list patch update watch]
  daemonsets.extensions                    []                 []              [create delete deletecollection get list patch update watch]
  deployments.extensions/rollback          []                 []              [create delete deletecollection get list patch update watch]
  deployments.extensions/scale             []                 []              [create delete deletecollection get list patch update watch]
  deployments.extensions                   []                 []              [create delete deletecollection get list patch update watch]
  ingresses.extensions                     []                 []              [create delete deletecollection get list patch update watch]
  networkpolicies.extensions               []                 []              [create delete deletecollection get list patch update watch]
  replicasets.extensions/scale             []                 []              [create delete deletecollection get list patch update watch]
  replicasets.extensions                   []                 []              [create delete deletecollection get list patch update watch]
  replicationcontrollers.extensions/scale  []                 []              [create delete deletecollection get list patch update watch]
  networkpolicies.networking.k8s.io        []                 []              [create delete deletecollection get list patch update watch]
  poddisruptionbudgets.policy              []                 []              [create delete deletecollection get list patch update watch]
  bindings                                 []                 []              [get list watch]
  events                                   []                 []              [get list watch]
  limitranges                              []                 []              [get list watch]
  namespaces/status                        []                 []              [get list watch]
  namespaces                               []                 []              [get list watch]
  pods/log                                 []                 []              [get list watch]
  pods/status                              []                 []              [get list watch]
  replicationcontrollers/status            []                 []              [get list watch]
  resourcequotas/status                    []                 []              [get list watch]
  resourcequotas                           []                 []              [get list watch]

```



##### admin

```sh
$ kubectl describe clusterrole admin
Name:         admin
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
PolicyRule:
  Resources                                       Non-Resource URLs  Resource Names  Verbs
  ---------                                       -----------------  --------------  -----
  serviceaccounts                                 []                 []              [create delete deletecollection get list patch update watch impersonate]
  configmaps                                      []                 []              [create delete deletecollection get list patch update watch]
  endpoints                                       []                 []              [create delete deletecollection get list patch update watch]
  persistentvolumeclaims                          []                 []              [create delete deletecollection get list patch update watch]
  pods/attach                                     []                 []              [create delete deletecollection get list patch update watch]
  pods/exec                                       []                 []              [create delete deletecollection get list patch update watch]
  pods/portforward                                []                 []              [create delete deletecollection get list patch update watch]
  pods/proxy                                      []                 []              [create delete deletecollection get list patch update watch]
  pods                                            []                 []              [create delete deletecollection get list patch update watch]
  replicationcontrollers/scale                    []                 []              [create delete deletecollection get list patch update watch]
  replicationcontrollers                          []                 []              [create delete deletecollection get list patch update watch]
  secrets                                         []                 []              [create delete deletecollection get list patch update watch]
  services/proxy                                  []                 []              [create delete deletecollection get list patch update watch]
  services                                        []                 []              [create delete deletecollection get list patch update watch]
  controllerrevisions.apps                        []                 []              [create delete deletecollection get list patch update watch]
  daemonsets.apps                                 []                 []              [create delete deletecollection get list patch update watch]
  deployments.apps/rollback                       []                 []              [create delete deletecollection get list patch update watch]
  deployments.apps/scale                          []                 []              [create delete deletecollection get list patch update watch]
  deployments.apps                                []                 []              [create delete deletecollection get list patch update watch]
  replicasets.apps/scale                          []                 []              [create delete deletecollection get list patch update watch]
  replicasets.apps                                []                 []              [create delete deletecollection get list patch update watch]
  statefulsets.apps/scale                         []                 []              [create delete deletecollection get list patch update watch]
  statefulsets.apps                               []                 []              [create delete deletecollection get list patch update watch]
  horizontalpodautoscalers.autoscaling            []                 []              [create delete deletecollection get list patch update watch]
  cronjobs.batch                                  []                 []              [create delete deletecollection get list patch update watch]
  jobs.batch                                      []                 []              [create delete deletecollection get list patch update watch]
  daemonsets.extensions                           []                 []              [create delete deletecollection get list patch update watch]
  deployments.extensions/rollback                 []                 []              [create delete deletecollection get list patch update watch]
  deployments.extensions/scale                    []                 []              [create delete deletecollection get list patch update watch]
  deployments.extensions                          []                 []              [create delete deletecollection get list patch update watch]
  ingresses.extensions                            []                 []              [create delete deletecollection get list patch update watch]
  networkpolicies.extensions                      []                 []              [create delete deletecollection get list patch update watch]
  replicasets.extensions/scale                    []                 []              [create delete deletecollection get list patch update watch]
  replicasets.extensions                          []                 []              [create delete deletecollection get list patch update watch]
  replicationcontrollers.extensions/scale         []                 []              [create delete deletecollection get list patch update watch]
  networkpolicies.networking.k8s.io               []                 []              [create delete deletecollection get list patch update watch]
  poddisruptionbudgets.policy                     []                 []              [create delete deletecollection get list patch update watch]
  rolebindings.rbac.authorization.k8s.io          []                 []              [create delete deletecollection get list patch update watch]
  roles.rbac.authorization.k8s.io                 []                 []              [create delete deletecollection get list patch update watch]
  localsubjectaccessreviews.authorization.k8s.io  []                 []              [create]
  bindings                                        []                 []              [get list watch]
  events                                          []                 []              [get list watch]
  limitranges                                     []                 []              [get list watch]
  namespaces/status                               []                 []              [get list watch]
  namespaces                                      []                 []              [get list watch]
  pods/log                                        []                 []              [get list watch]
  pods/status                                     []                 []              [get list watch]
  replicationcontrollers/status                   []                 []              [get list watch]
  resourcequotas/status                           []                 []              [get list watch]
  resourcequotas                                  []                 []              [get list watch]
```



## RBAC 소개

Role Based Access Control의 약자로 역할 기반 접근 제어로 풀이된다.

사용자(User, ServiceAccount, Group)의 역할(Role)을 보고 사용자가 액션(get, create, etc.)을 수행할 수 있는지 여부를 결정할 때 핵심 요소로 사용한다.

사용자는 하나 이상의 역할과 연관되며 각 역할은 특정 리소스에 대한 액션을 수행한다.

RBAC 인증 규칙은 4가지 리소스로 구성되며 2가지 그룹으로 묶을 수 있다.

| within a namespace    | cluster-wide                         |
| --------------------- | ------------------------------------ |
| 롤(Role)              | 클러스터롤(ClusterRole)              |
| 롤바인딩(RoleBinding) | 클러스터롤바인딩(ClusterRoleBinding) |

`롤(Role)` 은 **특정 API나 resources에 대해 취할 수 있는 액션(verbs)을 명시해둔 명세서**이며, `바인딩(Binding)` 은 **누가 이를 수행할 수 있는지를 정의**한다.

이를 도식화하면 아래와 같다.

**[ Pods에 대한 읽기 권한을 바인딩한 예 ]**

![Pods에 대한 읽기 권한을 바인딩한 예](바인딩한예.png)

| 계정 구분            | 설명                              |
| -------------------- | --------------------------------- |
| User                 | 개별 사용자                       |
| Service Account (SA) | Pod가 사용하는 시스템 계정        |
| Group                | 사용자 또는 ServiceAccount의 그룹 |



### 롤과 롤바인딩

먼저 Role은 액션의 범위(Scope)가 네임스페이스 내로 한정이 된다.

**[ Role 예시와 설명 ]**

```yaml
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: uptown
  name: pod-reader
rules:
  - apiGroups: ['']
    resources: ['pods']
    verbs: ['get', 'list']
```

> 한줄 요약 : (해당 롤을 바인딩한 사용자는) uptown 네임스페이스의 pods 리소스를 읽을 수 있다.

- kind: Role
  - 공통 설정으로 kind와 apiVersion을 명시한다.
- metadata.namespace: uptown / name: pod-reader
  - 이 Role이 속한 네임스페이스와 Role의 이름을 명시한다.
- rules:
  - 이 Role이 가지는 권한에 대해 기술한다.
- resources: ["pods"]
  - resources에는 어떤 자원에 접근 가능한지를 명시하는데 예시에서는 pods에만 접근 가능하다는 의미이다.
    (리소스 목록은 부록A 참조)
- verbs: ["get", "list"]
  - verbs는 어떤 동작이 가능한지를 설정하는 부분이다. 예시에서는 get과 list 권한만 가능하다는 의미이다.

**[ 설정 가능한 verbs ]**

| verbs            | 의미                          |
| ---------------- | ----------------------------- |
| create           | 새로운 리소스 생성            |
| get              | 개별 리소스 조회              |
| list             | 여러 건의 리소스 조회         |
| update           | 기존 리소스 전체 업데이트     |
| patch            | 기존 리소스 중 일부 내용 변경 |
| delete           | 개별 리소스 삭제              |
| deletecollection | 여러 리소스 삭제              |



### 클러스터롤과 클러스터롤바인딩

클러스터롤(ClusterRole)은 특정 네임스페이스에 국한되지 않는 **클러스터 전체에 대한 권한**을 관리한다. 그래서 <u>네임스페이스와 연관 없는 리소스</u>나 `/healthz` 와 같이 <u>리소스가 아닌 URL</u>에 대한 권한을 지정할 수 있다.

롤바인딩을 생성하고 클러스터롤을 참조하게 하여 네임스페이스의 리소스를 접근하게 할 수 있지만 클러스터 수준 리소스에 대한 접근 권한을 부여하려면 항상 클러스터 롤바인딩을 사용해야 한다.

**[ClusterRole 예시와 설명]**

```yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: read-clusterrole
rules:
  - apiGroups: ['']
    resources: ['pods']
    verbs: ['get', 'list']
```

> 한줄 요약 : (해당 롤을 바인딩한 사용자는) 클러스터 내 모든 네임스페이스의 pods 리소스를 읽을 수 있다.

- kind: ClusterRole
  - 공통설정으로 kind와 apiVersion을 나타낸다. (단, kind 부분이 ClusterRole로 되어 있다.)
- metadata.namespace
  - 특정 네임스페이스에 한정되지 않기 때문에 namespace 부분이 생략되어 있다.
- 그 외는 Role 예시와 같으나 의미는 클러스터 내 전체 네임스페이스의 포드에 대해 get, list가 가능함을 나타낸다.



### 디폴트 클러스터롤과 클러스터롤바인딩

쿠버네티스는 자체적으로 API 서버가 시작될 때마다 업데이트 되는 클러스터롤 및 클러스터롤바인딩의 **기본 세트**를 제공한다.

```sh
$ kubectl get clusterrolebindings
cluster-admin
system:basic-user
system:controller:attachdetach-controller
...

$ kubectl get clusterroles
cluster-admin
system:basic-user
system:discovery
admin
edit
view
...
```

이렇게 기본 세트의 클러스터롤을 사용하는 이유로 사용자 실수에 의한 삭제를 막을 수 있기 때문이다. 또한 새로운 버전의 쿠버네티스가 클러스터롤 및 바인딩의 다른 구성을 사용하는 경우 모든 디폴트 롤과 바인딩이 다시 만들어지기 때문에 사용자가 일일이 맞춰줄 필요가 없기 때문이다.

필요 시 서비스 계정(ServiceAccount)을 생성하고 클러스터 롤 바인딩(ClusterRoleBinding)을 통해 권한을 부여한다.
